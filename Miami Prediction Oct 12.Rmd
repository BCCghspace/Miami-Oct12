---
title: "Miami Home Sale Price Prediction"
author: "Anna Duan, Bingchu Chen"
date: "10/16/2020"
output: 
    html_document: 
        code_folding: hide
        toc: true
        toc float: true
        number_sections: true
---
# Introduction
In this report, we propose a new hedonic model for Zillow's housing market predictions. Accurate prediction of home sale prices is important right now as the real estate market is seeing record levels of activity due to the pandemic. Prediction future home sale prices is challenging due to the number of factors which affect the real estate market and the non-linear relationship between certain factors and prices. In this model, which is built for Miami and Miami Beach, we incorporate local intelligence from open sourced data to adapt it to local housing and development patterns. We use determinants of home prices including internal characteristics, nearby amenities and dis-amenities, and spatial processes such as clustering to estimate home sale prices. Applying this model to a set of 3503 houses, it predicted that home sale prices are highest on the shoreline, and are higher in Miami Beach than in Miami.
```{r Setup, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
knitr::opts_chunk$set

####load libraries, etc
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(knitr)
library(gridExtra)
library(ggcorrplot)
library(stargazer)
library(mapview)
library(osmdata)
library(tidycensus)
library(tidygeocoder)
library(raster)
library(rnaturalearth)
library(RColorBrewer)
library(rnaturalearthdata)
library(geosphere)

mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  )
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

palette5 <- c("#c8ddfa", "#8cb5ed", "#5890db",   "#2868bd", "#023578")

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}

#nearest neighbor function
nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>%
    dplyr::select(-thisPoint) %>%
    pull()

  return(output)  
}
```

# Data
For this analysis, we received a dataset of houses and their internal characteristics including number of rooms, living area, and pools. In addition, we gathered open sourced data from the American Community Survey, Miami Dade County's Open Data Hub, OpenStreetMaps. This includes  census-tract level demographic information and point and polygon data of amenities including restaurants and parks (Figure 1). It is expected that attributes that contribute to quality of life such as proximity to restaurants, park space, and low commuting times correlate with higher sale prices.
```{r Read Data, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
#projected to NAD 1983 StatePlane Florida East FIPS 0901 Feet



#STUDY AREA
#miamiBound <- st_read("E:/Upenn/CPLN508/miami/2_Miami-Prediction/Raw Data/Municipal_Boundary.geojson") %>%
miamiBound <- st_read("/Users/annaduan/Documents/GitHub/2_Miami\ Prediction/Raw\ Data/Municipal_Boundary.geojson") %>%
  filter(NAME == "MIAMI BEACH" | NAME == "MIAMI") %>%
  st_union() %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')



#STUDY AREA OSM (not projected so that it works)
#miamiBoundOSM <- st_read("E:/Upenn/CPLN508/miami/2_Miami-Prediction/Raw Data/Municipal_Boundary.geojson") %>%
miamiBoundOSM <- st_read("/Users/annaduan/Documents/GitHub/2_Miami\ Prediction/Raw\ Data/Municipal_Boundary.geojson") %>%
  filter(NAME == "MIAMI BEACH" | NAME == "MIAMI") %>%
  st_union()



#HOUSE DATA
#houses <- st_read("E:/Upenn/CPLN508/miami/2_Miami-Prediction/Raw Data/studentsData.geojson") %>%
houses <- st_read("/Users/annaduan/Documents/GitHub/2_Miami\ Prediction/Raw\ Data/studentsData.geojson") %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
  st_centroid()




#HOUSE DATA OSM (Not projected)
#housesOSM <- st_read("E:/Upenn/CPLN508/miami/2_Miami-Prediction/Raw Data/studentsData.geojson")
housesOSM <- st_read("/Users/annaduan/Documents/GitHub/2_Miami\ Prediction/Raw\ Data/studentsData.geojson")



#CENSUS
census_api_key("d9ebfd04caa0138647fbacd94c657cdecbf705e9", install = TRUE, overwrite = TRUE)
#read in: vacant property, total housing units, mhhinc, white, population, owner occ, renter occ, travel time to work
acs <-
  get_acs(geography = "tract", variables = c("B25002_003E", "B25001_001E", "B19013_001E", "B01001A_001E", "B01003_001E", "B07013_002E", "B07013_003E", "B08012_001E", "B25104_001E"), year=2018, state=12, county=086, geometry=T) %>%
  st_transform('ESRI:102658')
#filter for Miami/Miami beach tracts
acs <-
  rbind(
    st_centroid(acs)[miamiBound,] %>%
      st_drop_geometry() %>%
      left_join(acs) %>%
      st_sf() %>%
      mutate(inMiami = "YES"),
    st_centroid(acs)[miamiBound, op = st_disjoint] %>%
      st_drop_geometry() %>%
      left_join(acs) %>%
      st_sf() %>%
      mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(-inMiami)
#long to wide form
acs <-
  acs %>%
  dplyr::select(-moe, -GEOID) %>%
  spread(variable, estimate) %>%
  dplyr::select(-geometry) %>%
  rename(vacantUnits = B25002_003,
         totalUnits = B25001_001,
         medHHInc = B19013_001,
         white = B01001A_001,
         population = B01003_001,
         ownerOcc = B07013_002,
         renterOcc = B07013_003,
         timeToWork = B08012_001,
         monthhousingcost = B25104_001)
acs["119", "medHHInc"] = 33194   #input value from nearby tract in same neighborhod because NA was messing up MAE
acs %>% na.omit()


#mutate
acs <-
  acs %>%
  mutate(pctVacant = ifelse(totalUnits > 0, vacantUnits / totalUnits, 0),
         pctWhite = ifelse(population > 0, white / population, 0),
         totalOcc = ownerOcc + renterOcc,
         pctRenterOcc = ifelse(totalOcc > 0, renterOcc / totalOcc, 0)) %>%
  dplyr::select(-totalUnits,-vacantUnits,-totalUnits,-population,-white, -ownerOcc, -renterOcc, -totalOcc)


#OSM BBOX (uses the non-projected base)
xmin = st_bbox(miamiBoundOSM)[[1]]
ymin = st_bbox(miamiBoundOSM)[[2]]
xmax = st_bbox(miamiBoundOSM)[[3]]  
ymax = st_bbox(miamiBoundOSM)[[4]]



#FOOD AND BEVERAGE SPOTS
 foodBev <- opq(bbox = c(xmin, ymin, xmax, ymax)) %>%
  add_osm_feature(key = 'amenity', value = c("bar","pub","restaurant","cafe")) %>%
   osmdata_xml(filename = 'foodBev.osm')
 #project
 foodBev <- sf::st_read('foodBev.osm', layer = 'points') %>%
     st_as_sf(coords = c("LON", "LAT"), crs = EPSG:3857, agr = "constant") %>%
  st_transform('ESRI:102658')
 #filter for facilities in study area
 foodBev <- rbind(
    st_centroid(foodBev)[miamiBound,] %>%
      st_drop_geometry() %>%
      left_join(foodBev) %>%
      st_sf() %>%
      mutate(inMiami = "YES"),
    foodBev[miamiBound, op = st_disjoint] %>%
      st_drop_geometry() %>%
      left_join(foodBev) %>%
      st_sf() %>%
      mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
   dplyr::select(name)



#COASTLINE
Coastline<-opq(bbox = c(xmin, ymin, xmax, ymax)) %>%
  add_osm_feature("natural", "coastline") %>%
  osmdata_sf()
#add to housesOSM and convert to miles, then add to houses
housesOSM <-
  housesOSM %>%  
  mutate(CoastDist=(geosphere::dist2Line(p=st_coordinates(st_centroid(housesOSM)),
                                        line=st_coordinates(Coastline$osm_lines)[,1:2])*0.00062137)[,1])
houses <-
  houses %>%
  mutate(distWater = housesOSM$CoastDist,
         SPSqFt = ifelse(!is.na(ActualSqFt)&!is.na(SalePrice), SalePrice / ActualSqFt, 0))



#PARKS
muniParks <- st_read("https://opendata.arcgis.com/datasets/16fe02a1defa45b28bf14a29fb5f0428_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
  dplyr::select(NAME, ADDRESS, CITY, CLASS, Shape__Area)

countyParks <- st_read("https://opendata.arcgis.com/datasets/aca1e6ff0f634be282d50cc2d534a832_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
    dplyr::select(NAME, ADDRESS, CITY, CLASS, Shape__Area)
parks <- bind_rows(muniParks, countyParks) %>%
  filter(CITY == "Miami" | CITY == "Miami Beach") %>%
  mutate(counter = 1)



#SCHOOL DISTRICT
schoolDist <- st_read("https://opendata.arcgis.com/datasets/bc16a5ebcdcd4f3e83b55c5d697a0317_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
  dplyr::select(ID)



#PUBLIC SCHOOL CATCHMENT/ATTENDANCE ZONES
#elementary
elementary <- st_read("https://opendata.arcgis.com/datasets/19f5d8dcd9714e6fbd9043ac7a50c6f6_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
elementary <- rbind(
  st_centroid(elementary)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(elementary) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(elementary)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(elementary) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(NAME)
#middle
middle <- st_read("https://opendata.arcgis.com/datasets/dd2719ff6105463187197165a9c8dd5c_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
middle <- rbind(
  st_centroid(middle)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(middle) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(middle)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(middle) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(NAME)
#high
high <- st_read("https://opendata.arcgis.com/datasets/9004dbf5f7f645d493bfb6b875a43dc1_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
high <- rbind(
  st_centroid(high)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(high) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(high)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(high) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
  dplyr::select(NAME)



#PUBLIC TRANSPORTATION
#bus
bus <- st_read("https://opendata.arcgis.com/datasets/021adadcf6854f59852ff4652ad90c11_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant")  %>%
  st_transform('ESRI:102658')
bus <- rbind(
  bus[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(bus) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  bus[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(bus) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")
#metro mover
metromover <- st_read("https://opendata.arcgis.com/datasets/aec76104165c4e879b9b0203fa436dab_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
metromover <- rbind(
  metromover[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(metromover) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  metromover[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(metromover) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")
#metro rail
metrorail <- st_read("https://opendata.arcgis.com/datasets/ee3e2c45427e4c85b751d8ad57dd7b16_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
metrorail <- rbind(
  metrorail[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(metrorail) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  metrorail[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(metrorail) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")



#CULTURE SPOTS
culture <- st_read("https://opendata.arcgis.com/datasets/70c48f0eb067448c8a787cfa1c1c3bb9_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
culture <- rbind(
  culture[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(culture) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  culture[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(culture) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")



#COMMERCIAL PROPERTIES
#read, project
commercial <- st_read("https://opendata.arcgis.com/datasets/fb8303c577c24ea386a91be7329842be_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
#filter
commercial <- rbind(
  commercial[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(commercial) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  commercial[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(commercial) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")



#FLOOD RISK ZONES
floodRisk <- st_read("https://opendata.arcgis.com/datasets/ef3bdd041b2e424695eb4dfe965966c4_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
#filter
 floodRisk <-
   rbind(
  st_centroid(floodRisk)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(floodRisk) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(floodRisk)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(floodRisk) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
   dplyr::select(-inMiami, -SHAPE_Length, -ELEV, -FID) %>%
   dplyr::rename(FloodZone = FZONE, FloodHazard = ZONESUBTY)


 floodInsure <- st_read("https://opendata.arcgis.com/datasets/f589473ddada46e78d437aaf09205b04_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
filter
 floodInsure <-
   rbind(
  st_centroid(floodInsure)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(floodInsure) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(floodInsure)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(floodInsure) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES") %>%
   mutate(floodInsureType = PANELID)



#CONTAMINATED SITES
contaminated <- st_read("https://opendata.arcgis.com/datasets/43750f842b1e451aa0347a2ca34a61d7_0.geojson") %>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
 contaminated <-
   rbind(
  st_centroid(contaminated)[miamiBound,] %>%
    st_drop_geometry() %>%
    left_join(contaminated) %>%
    st_sf() %>%
    mutate(inMiami = "YES"),
  st_centroid(contaminated)[miamiBound, op = st_disjoint] %>%
    st_drop_geometry() %>%
    left_join(contaminated) %>%
    st_sf() %>%
    mutate(inMiami = "NO")) %>%
  filter(inMiami == "YES")



#low income depressed area (county level no need to clip)
 #AD: Are these within our study area? I don't see much overlap and I feel this would result in a lot of NAs
#low_income_depressed <- st_read("https://opendata.arcgis.com/datasets/40119bfc50274c1da548ec8022e9a7a9_0.geojson") %>%
#  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
#  st_transform('ESRI:102658')
#low_income_depressed <- rbind(
#  commercial[miamiBound,] %>%
#    st_drop_geometry() %>%
#    left_join(low_income_depressed) %>%
#    st_sf() %>%
#    mutate(inMiami = "YES"),
#  commercial[miamiBound, op = st_disjoint] %>%
#    st_drop_geometry() %>%
#    left_join(low_income_depressed) %>%
#    st_sf() %>%
#    mutate(inMiami = "NO")) %>%
#  filter(inMiami == "YES")

#neighborhood_revitalization <- st_read("https://opendata.arcgis.com/datasets/fe6f419e21264158b18eb77be9870d97_0.geojson") #%>%
#  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
#  st_transform('ESRI:102658')
#neighborhood_revitalization <- rbind(
#  commercial[miamiBound,] %>%
#    st_drop_geometry() %>%
#    left_join(neighborhood_revitalization) %>%
#    st_sf() %>%
#    mutate(inMiami = "YES"),
#  commercial[miamiBound, op = st_disjoint] %>%
#    st_drop_geometry() %>%
#    left_join(neighborhood_revitalization) %>%
#    st_sf() %>%
#    mutate(inMiami = "NO")) %>%
#  filter(inMiami == "YES")

#neighborhood stability https://gis-mdc.opendata.arcgis.com/datasets/neighborhood-stabilization-program/data
#nbh_stability <- st_read("https://opendata.arcgis.com/datasets/5c0822e7c26d437dbc04103ddf05d2fc_0.geojson") %>%
#  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
#  st_transform('ESRI:102658')

#install.packages("rmapshaper")
#library(rmapshaper)

#nbh_sta <- ms_clip(  #AD: what does this do?
#  nbh_stability,
#  clip = NULL,
#  bbox = c(xmin, ymin, xmax, ymax),
#  remove_slivers = FALSE,
#  force_FC = TRUE,
#  sys = FALSE
#)

#redevelopment area
#redev <- st_read("https://opendata.arcgis.com/datasets/38d923b6509547f8bde102e621100b53_0.geojson") %>%
#  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
#  st_transform('ESRI:102658')
#redev <- rbind(
#  commercial[miamiBound,] %>%
#    st_drop_geometry() %>%
#    left_join(redev) %>%
#    st_sf() %>%
#    mutate(inMiami = "YES"),
#  commercial[miamiBound, op = st_disjoint] %>%
#    st_drop_geometry() %>%
#    left_join(redev) %>%
#    st_sf() %>%
#    mutate(inMiami = "NO")) %>%
#  filter(inMiami == "YES")

#1shop
#BC:try this?
#AD: worked!
#shop_m <- st_read("/Users/annaduan/Documents/GitHub/2_Miami\ Prediction/Raw\ Data/shop_point.geojson") %>%
#shop_m <- st_read("E:/Upenn/CPLN508/miami/2_Miami-Prediction/Raw Data/shop_point.geojson") %>%
#  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
#  st_transform('ESRI:102658')
#shop_mb <- st_read("/Users/annaduan/Documents/GitHub/2_Miami\ Prediction/Raw\ Data/shop_point_beach.geojson") %>%
#shop_mb <- st_read("E:/Upenn/CPLN508/miami/2_Miami-Prediction/Raw Data/shop_point_beach.geojson") %>%
#  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
#  st_transform('ESRI:102658')
#sub_shop_m <- shop_m[, c("full_id", "osm_id", "osm_type", "name", "shop", "addr.city", "addr.street")]
#sub_shop_mb <- shop_mb[, c("full_id", "osm_id", "osm_type", "name", "shop", "addr.city", "addr.street")]
#shop <- rbind(sub_shop_m, sub_shop_mb)

#nece_shop <- shop %>% filter(shop %in% c("convenience", "supermarket", "department_store", "bakery", "greengrocer", "General Store"))  
#poor_shop <- shop %>% filter(shop %in% c("variety_store", "charity", "second_hand"))
#n_nece_shop <- setdiff(shop, rbind(nece_shop, poor_shop))
#n_nece_shop <- n_nece_shop[!(n_nece_shop$shop == "" | is.na(n_nece_shop$shop) | n_nece_shop$shop == "vacant"), ]

```


```{r Fig 1: TABLE summary stats, message=FALSE, warning=FALSE, include=TRUE}
#table of summary statistics with variable descriptions, sorted by category

housesSub <- houses %>%
  dplyr::select("AdjustedSqFt", "LotSize", "Bed", "Bath", "Stories", "commercialProperties", "distWater", "foodEstablishments", "cultureSpots", "busStops", "parkArea", "timeToWork","monthhousingcost","pctVacant")

st_drop_geometry(housesSub)
stargazer(as.data.frame(housesSub), type="text", digits=1, title="Descriptive Statistics for Miami Houses", out = "Miami Data.txt")

```

```{r Wrangle Data, message=FALSE, warning=FALSE, include=TRUE, results='hide'}

#CONTAMINATION BUFFER

 contamBuffer <- contaminated %>%
   st_buffer(800) %>%
   st_union() %>%
   st_as_sf() %>%
   mutate(contam = 1)
 houses$contaminated <- houses %>%
   st_join(contamBuffer) %>%
   mutate(contam = ifelse(is.na(contam), 0, 1)) %>%
   pull(contam)



#NEAREST NEIGHBOR (some are used for testing, to determine feature buffer distances)
 st_c <- st_coordinates
 houses <-
   houses %>%
   mutate(
     #commercial properties NN
     commNN1 = nn_function(st_c(st_centroid(houses)), st_c(st_centroid(commercial)), 1),
     commNN5 = nn_function(st_c(st_centroid(houses)), st_c(st_centroid(commercial)), 5),
     #metro mover stations
     metroMNN1 = nn_function(st_c(st_centroid(houses)), st_c(metromover), 1),
     metroMNN5 = nn_function(st_c(st_centroid(houses)), st_c(metromover), 5),
     #metro rail stations
     metroRNN1 = nn_function(st_c(st_centroid(houses)), st_c(metrorail), 1),
     metroRNN5 = nn_function(st_c(st_centroid(houses)), st_c(metrorail), 5),
     #food/drinks
     foodBevNN1 = nn_function(st_c(st_centroid(houses)), st_c(foodBev), 1),
     foodBevNN5 = nn_function(st_c(st_centroid(houses)), st_c(foodBev), 5),
     #daily food shopping place 5250.894
     neceshopNN1 = nn_function(st_c(st_centroid(houses)), st_c(nece_shop), 1),

     neceshopNN5 = nn_function(st_c(st_centroid(houses)), st_c(nece_shop), 5)
     ) 


#COMMERCIAL BUFFER
 commercial <- commercial %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count properties within each buffer
houses$commercialProperties <-
   st_buffer(houses, 846) %>%
   aggregate(commercial, ., sum) %>%
   st_drop_geometry() %>%
  mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#FOOD AND BEV BUFFER
 foodBev <- foodBev %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count parks within each buffer
houses$foodEstablishments <-
   st_buffer(houses, 2774) %>%
   aggregate(foodBev, ., sum) %>%
   st_drop_geometry() %>%
    mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#CULTURE BUFFER
 culture <- culture %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count culture within each buffer
houses$cultureSpots <-
   st_buffer(houses, 774) %>%
   aggregate(culture, ., sum) %>%
   st_drop_geometry() %>%
   mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)




#METRORAIL BUFFER
 metrorail <- metrorail %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count stops within each buffer
houses$metrorailStops <-
   st_buffer(houses, 12076.7) %>%
   aggregate(metrorail, ., sum) %>%
   st_drop_geometry() %>%
  mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#METROMOVER BUFFER
 metromover <- metromover %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count metroM stops within each buffer
houses$metromoverStops <-
   st_buffer(houses, 18845) %>%
   aggregate(metromover, ., sum) %>%
   st_drop_geometry() %>%
   mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



#BUS BUFFER
 bus <- bus %>%
   mutate(counter = 1) %>%
   dplyr::select(counter)
 #count bus within each buffer
houses$busStops <-
   st_buffer(houses, 775) %>%
   aggregate(bus, ., sum) %>%
   st_drop_geometry() %>%
  mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)



 #PARKS BUFFER + AREA CALCULATION (using 1600ft buffer distance because the mean NN1 = 1600)
 #get centroids
 parkCentroids <- parks %>%
   st_centroid(parks) %>%    #get centroids of park layer
  dplyr::select(counter)
 #count parks within each buffer
houses$parkCount <-
   st_buffer(houses, 1600) %>%
   aggregate(parkCentroids, ., sum) %>%
   st_drop_geometry() %>%
   mutate(counter = ifelse(is.na(counter), 0, counter)) %>%
   pull(counter)
#make buffer for each house
parkBuffer <- st_buffer(houses, 1600) %>%
  dplyr::select(Property.Address) %>%
  st_as_sf()
#calculate area of park space in each buffer
bufferedParks <- st_intersection(parkBuffer, parks) %>%
  group_by(Property.Address) %>%
  summarise() %>%
  mutate(parkArea = units::drop_units(st_area(.))) %>%
  st_drop_geometry()
#add park area back to houses file
houses <-
  left_join(houses, bufferedParks)



#SCHOOL CATCHMENT CATEGORIES
 houses <-
   st_join(houses, elementary) %>%
   rename(elemCatch = 'NAME')

  houses <-
   st_join(houses, middle) %>%
   rename(middleCatch = 'NAME')

   houses <-
   st_join(houses, high) %>%
   rename(highCatch = 'NAME')



 #SCHOOL DISTRICT CATEGORIES
 houses <-
   st_join(houses, schoolDist) %>%
   rename(schoolDist = ID)


 #FLOOD INSURANCE CATEGORIES
floodInsure <- floodInsure %>%
  dplyr::select(floodInsureType)
houses <- houses %>%
  st_join(., floodInsure) %>%
  mutate(floodInsureType = ifelse(is.na(floodInsureType), "other", floodInsureType))



#ADD ACS DATA
houses <-
  st_join(houses, acs)




 #HOUSE AGE
houses <-
   houses %>%
   mutate(age = ifelse(is.na(YearBuilt), 0, (2020 - YearBuilt)))



 #MAKE CATEGORICAL VARIABLES
 houses <-
  houses %>%
  mutate(Bed.cat = case_when(
                  Bed >= 0 & Bed < 3  ~ "Up to 2 Beds",
                  Bed >= 3 & Bed < 4  ~ "3 Beds",
                  Bed >= 4                    ~ "4+ Beds"))


 houses <-
  houses %>%
  mutate(Bath.cat = case_when(
                  Bath < 2  ~ "Up to 1 Bathroom",
                  Bath == 2  ~ "2 Bathrooms",
                  Bath >= 3                    ~ "3+ Bathrooms"))


 houses <-
  houses %>%
  mutate(Stories.cat = case_when(
                  Stories < 2  ~ "Up to 1 Stories",
                  Stories == 2  ~ "2 Stories",
                  Stories >= 3                    ~ "3+ Stories"))



 houses <-
  houses %>%
  mutate(distWater.cat = case_when(
                  distWater < 0.25  ~ "Less than 1/4 Mile",
                  distWater >= 0.25   ~ "More than 1/4 Mile"))


 houses <-
  houses %>%
  mutate(parkArea.cat = case_when(
                  parkArea < 57000  ~ "Less than 57,000 SqFt",
                  parkArea >= 57000 & parkArea < 320000  ~ "57,000 - 320,000 SqFt",
                  parkArea >= 320000              ~ "More than 320,000 SqFt"))


 qplot(houses$parkArea, geom="histogram") +
   plotTheme()

 #OTHER HOUSE FEATURES
houses <- houses %>%
  mutate(Pool = ifelse(str_detect(XF1, "Pool") | str_detect(XF2, "Pool") | str_detect(XF3, "Pool") | str_detect(XF1, "Whirlpool") | str_detect(XF2, "Whirlpool") | str_detect(XF3, "Whirlpool") | str_detect(XF1, "Jacuzzi") | str_detect(XF2, "Jacuzzi") | str_detect(XF3, "Jacuzzi"), "Pool", "No Pool"),
         Patio = ifelse(str_detect(XF1, "Patio") | str_detect(XF2, "Patio") | str_detect(XF3, "Patio"), "Patio", "No Patio"),
         Fence = ifelse(str_detect(XF1, "Fence") | str_detect(XF2, "Fence") | str_detect(XF3, "Fence"), "Fence", "No Fence"),
         Gazebo = ifelse(str_detect(XF1, "Gazebo") | str_detect(XF2, "Gazebo") | str_detect(XF3, "Gazebo"), "Gazebo", "No Gazebo"),
         Carport = ifelse(str_detect(XF1, "Carport") | str_detect(XF2, "Carport") | str_detect(XF3, "Carport"), "Carport", "No Carport"),
         Wall = ifelse(str_detect(XF1, "Wall") | str_detect(XF2, "Wall") | str_detect(XF3, "Wall"), "Wall", "No Wall"),
         Dock = ifelse(str_detect(XF1, "Dock") | str_detect(XF2, "Dock") | str_detect(XF3, "Dock"), "Dock", "No Dock"),
         )

#FIX NA VALUES
#zip
 houses <-
  houses %>%
  mutate(Mailing.Zip = as.numeric(Mailing.Zip),
         Mailing.Zip = ifelse(is.na(Mailing.Zip), 0, Mailing.Zip))

#School dist
 houses <-
   houses %>%
  mutate(elemCatch = ifelse(is.na(elemCatch), "other", elemCatch),
         middleCatch = ifelse(is.na(middleCatch), "other", middleCatch),
         highCatch = ifelse(is.na(highCatch), "other", highCatch),
         )



#park
 houses <- houses %>%
     mutate(parkArea = ifelse(is.na(parkArea), 0, parkArea))

 #acs - here I found the location of NA values and gave these houses the ACS values of houses nearby/in same neighborhood
houses["3487", "NAME"] = houses["1099", "NAME"]
houses["3487","timeToWork"] =  houses["1099","timeToWork"]
houses["3487", "medHHInc"] = houses["1099", "medHHInc"]
houses["3487", "monthhousingcost"] = houses["1099", "monthhousingcost"]
houses["3487", "pctVacant"] = houses["1099", "pctVacant"]
houses["3487", "pctWhite"] = houses["1099", "pctWhite"]
houses["3487", "pctRenterOcc"] = houses["1099", "pctRenterOcc"]


houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "NAME"] = houses["3458", "NAME"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "timeToWork"] = houses["3458", "timeToWork"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "medHHInc"] = houses["3458", "medHHInc"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "monthhousingcost"] = houses["3458", "monthhousingcost"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "pctVacant"] = houses["3458", "pctVacant"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "pctWhite"] = houses["3458", "pctWhite"]
houses[c("579","580","581","1016","1372","1557","1853","2140","2557","2563","2571","2603","2786","2981","3050","3361"), "pctRenterOcc"] = houses["3458", "pctRenterOcc"]

houses["441", c("NAME","timeToWork","medHHInc","monthhousingcost","pctVacant","pctWhite","pctRenterOcc")] = houses["2090", c("NAME","timeToWork","medHHInc","monthhousingcost","pctVacant","pctWhite","pctRenterOcc")]

houses %>%
  dplyr::select(-Property.Zip)



```

```{r Fig 2: Correlation Matrix, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}

corrPlotVars <- houses %>%
  dplyr::select(-saleDate, -saleType, -saleYear, -Bldg, -Land, -Assessed, -WVDB, -HEX, -GPAR, -County.2nd.HEX, -County.Senior, -County.LongTermSenior, -County.Other.Exempt, -County.Taxable, -City.2nd.HEX, -City.Senior, -City.LongTermSenior, -City.Other.Exempt, -City.Taxable, -MillCode, -Owner1, -Owner2, -Mailing.State, -Mailing.Country, -Legal1, -Legal2, -Legal3, -Legal4, -Legal5, -Legal6, -YearBuilt, -EffectiveYearBuilt, -toPredict)

numericVars <-
  select_if(st_drop_geometry(corrPlotVars), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1),
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Figure 2: Sale Price Correlation with Numeric Variables") +
  plotTheme()

```


Three variables that we will explore further are distWater, parkArea, pctVacant, and busStops. We expect that higher distWater and pctVacant values correlate with low sale prices, and parkArea and busStops make nearby houses more expensive and cost more.

	As expected, proximity to the shoreline (low distWater) is desirable, and correlates with higher sale price. Based on figure 3.1, however, this trend is strongest when distWater < 1. This suggests that we need to feature engineer distWater to account for greater correlation at shorter distances.
	The amount of park area near a house is positively correlated with sale price, although not as strongly as expected. 
	
	Surprisingly, the share of a census tract that is vacant has a positive correlation with sale price. This is unexpected because typically, vacant houses correlate with neighborhood disorder and unattractiveness. However, it is possible that these vacancies are the result of new construction and therefore do not make the area less attractive. 
	Bus stops also defy our understanding of cities: according to TOD theory, homes near transportation should be more attractive. However, as figure 3.3 shows, the number of nearby bus stops correlates negatively with sale price.
```{r Fig 3: 4 SCATTERPLOT home price correlation, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}

#1: distWater
ggplot(data = housesKnown, aes(x = distWater, y = SalePrice)) +
  geom_point(size=2, shape=20)  +
  labs(title = "Figure 3.1: Distance to Shoreline and Sale Price", subtitle = "Miami and Miami Beach, FL") +
  geom_smooth(method = "lm", se=F, colour = "green") +
  plotTheme()

#2: parkArea
ggplot(data = housesKnown, aes(x = parkArea, y = SalePrice)) +
  geom_point(size=2, shape=20) +
  labs(title = "Figure 3.2: Nearby Park Space and Sale Price", subtitle = "Miami and Miami Beach, FL") +
  geom_smooth(method = "lm", se=F, colour = "green") +
    plotTheme()

#3: bus stops
ggplot(data = housesKnown, aes(x = busStops, y = SalePrice)) +
  geom_point(size=2, shape=20) +
 labs(title = "Figure 3.3: Bus Stops and Sale Price", subtitle = "Miami and Miami Beach, FL") +
  geom_smooth(method = "lm", se=F, colour = "green") +
    plotTheme()

#4: pctRenter
ggplot(data = housesKnown, aes(x = pctVacant, y = SalePrice)) +
  geom_point(size=2, shape=20) +
 labs(title = "Figure 3.4: Share of Vacant Properties and Sale Price", subtitle = "Miami and Miami Beach, FL") +
  geom_smooth(method = "lm", se=F, colour = "green") +
    plotTheme()

```
Another method of exploring spatial patterns of variables and their correlation with sale price is by mapping. Figure 4 shows the distribution of known sale prices. Prices are highest in Miami Beach and the shoreline of Miami. It gets progressively lower until it reaches its lowest cluster in the North. This suggests that as expected, the shoreline area is attractive to homebuyers.

```{r Fig 4: MAP Sale Price, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}
#map of your dependent variable (sale price)
  #water is just for mapping visuals
water <- st_read("https://opendata.arcgis.com/datasets/bf9de3192c9c4e458d1453f6d4c88d6c_0.geojson") %>%
 st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658') %>%
  st_union() %>%
  st_intersection(.,miamiBound)

ggplot() +
  geom_sf(data = acs, fill = "gray80", colour = "white") +
  geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = housesKnown, aes(colour = q5(SalePrice))) +
  scale_colour_manual(values = paletteMap) +
  labs(title = "Figure 4: Home Sale Price", subtitle = "Miami and Miami Beach, FL") +
  mapTheme()
```

To further investigate the spatial distribution of sale prices, we will map distWater, AdjustedSqFt, MedHHInc, parkArea, and foodEstablishments. All of these variables should correlate positively with sale price, and therefore show similar patterns.

It can be expected that distance to water is negatively correlated to sale price in a city known for its beaches. Figure 5.1 shows that distance to the shoreline closely matches sale price. 

However, it's possible that sale price appears to be strongly correlated with distance to water because of other factors. Indeed, the downtown area of Miami is located near the beach, and the increased development around the beaches and downtown may make the area attractive for other reasons.

Two of these factors are the amount of green space and dining venues near a house. Figure 5.2 shows that houses in Miami Beach have the most green space within a 1600meter buffer. Interestingly, only some of the high sale price houses have high park area, mostly in Miami Beach. In Figure 5.4, we see similar patterns, with the highest values in the houses in Miami Beach and the north part of Miami's shoreline. This suggests that distance to the shoreline is only part of the story.

Outside of amenities, it can be expected that sale price can be predicted with median household income of census tracts and the adjusted square footage of houses. In Figures 5.4 and 5.5, we see this confirmed: much like sale price, the highest values for income and square footage are along the shoreline. 
```{r Fig 5: MAP 3 Independent Variables, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}

#1: distWater
ggplot() +
geom_sf(data = acs, fill = "gray90", colour = "white") +
  geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = houses, aes(colour = q5(distWater))) +
  labs(title = "Figure 5.1: Distance to Shore", subtitle = "Miami and Miami Beach, FL") +
  scale_colour_manual(values = palette5) +
  mapTheme()

#2: parkArea
ggplot() +
  geom_sf(data = acs, fill = "gray90", colour = "white") +
  geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = housesKnown, aes(colour = q5(parkArea))) +
  scale_colour_manual(values = palette5) +
  labs(title = "Figure 5.2: Nearby Park Area", subtitle = "Miami, FL") +
  mapTheme()


#3: Food and bev
ggplot() +
  geom_sf(data = acs, fill = "gray90", colour = "white") +
  geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = housesKnown, aes(colour = q5(foodEstablishments))) +
  scale_colour_manual(values = palette5) +
  labs(title = "Figure 5.3: Number of Nearby Food/Bev Places", subtitle = "Miami, FL") +
  mapTheme()


#4: AdjustedSqFt
  ggplot() +
geom_sf(data = acs, fill = "gray90", colour = "white") +
  geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = houses, aes(colour = q5(AdjustedSqFt))) +
  labs(title = "Figure 5.4: Adjusted Square Feet", subtitle = "Miami and Miami Beach, FL") +
  scale_colour_manual(values = palette5) +
  mapTheme()

#5: MedHHInc
  ggplot() +
geom_sf(data = acs, fill = "gray90", colour = "white") +
  geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = houses, aes(colour = q5(medHHInc))) +
  labs(title = "Figure 5.5: Tract Median Household Income", subtitle = "Miami and Miami Beach, FL") +
  scale_colour_manual(values = palette5) +
  mapTheme()

```

# Methods 
Now knowing that attractive amenities, high income households, large houses, and high sale prices cluster around the shoreline area, we can begin to test features to put in our regression model. Using cor.test, we ranked the correlation of each feature with SalePrice, then added them in order until each added feature no longer increased the model's R^2, or predictive power for sale price. We made multiple regression models and compared their mean average error in sale price prediction until we decided on regression 3 which includes house internal features, census tract variables, transportation availability, park area, and flood risk.

```{r Designing Regression, message=FALSE, warning=FALSE, include=TRUE, results='hide'}

 cor.test(houses$parkArea, houses$SalePrice, method = "pearson")

 #1: 0.71
 #reg1 <- lm(SalePrice ~ ., data = st_drop_geometry(houses) %>%
 #            dplyr::select(SalePrice, LivingSqFt, Bed.cat, Bath.cat, LotSize, Stories.cat, Zoning, age, timeToWork, parkArea, medHHInc, pctVacant, foodEstablishments, metromoverStops, Pool, Dock, Patio, floodInsureType, highCatch))
 #summary(reg1)

 #2: 0.72   - this one works better with training
  reg2 <- lm(SalePrice ~ ., data = st_drop_geometry(houses) %>%
             dplyr::select(SalePrice, ActualSqFt, LotSize, Zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType))
```

Next, to test this model, we used the houses that we know sale prices for to create a training set for training our model, and a test set for testing it. 
```{r Train the regression, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
#read FL neighborhoods
#nhoods_fl <- aoi_boundary_HARV <- st_read("E:/Upenn/CPLN508/miami/zillow_nghbrhd_feb17/zillow_nghbrhd_feb17.shp")
nhoods_fl <- aoi_boundary_HARV <- st_read("/Users/annaduan/Documents/GitHub/Miami-Oct12/Raw\ Data/zillow_nghbrhd_feb17/zillow_nghbrhd_feb17.shp")
nhoods_mb <- subset(nhoods_fl, CITY == "MIAMI BEACH")%>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')
nhoods_m <- subset(nhoods_fl, CITY == "MIAMI")%>%
  st_as_sf(coords = c("LON", "LAT"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

#Join neighborhoods
nhoods <- rbind(nhoods_mb, nhoods_m)
nhoods <- nhoods %>%
  dplyr::select(NAME) %>%
  rename(neighborhood = NAME)
houses <- houses %>% st_join(., nhoods, join = st_within)

#Separate by toPredict
housesKnown <- houses %>%    
  filter(.,toPredict == 0)
housesUnknown <- houses %>%
  filter(.,toPredict ==1)
#Make train and test sets
inTrain <- createDataPartition(
              y = paste(housesKnown$Zoning, housesKnown$floodInsureType, housesKnown$neighborhood),
              p = .60, list = FALSE)
miami.training <- housesKnown[inTrain,]
miami.test <- housesKnown[-inTrain,]  


#Training regression 
reg.training <- 
  lm(SalePrice ~ ., data = st_drop_geometry(miami.training) %>% 
                             dplyr::select(SalePrice, ActualSqFt, LotSize, Zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType))
```

# Results
The results of the regression model on the training data set are presented in Table 2. Overall, the R-squared value of 0.9 indicates that our variables explain a lot of the variation in sale price. The p values are also mostly below 0.05, indicating a high level of confidence in these results. The residual standard error, however, is still relatively high.
```{r Fig 6: TABLE Training Set lm, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
#polished table of  (training set) lm summary results (coefficients, R2 etc)
stargazer(reg.training, type="text", digits=1, title="Table 2: LM of Training Data", out = "Training LM.txt")
```

Next, we test the regression model on our test set to see its quality when used to predict on data it hasn't seen before. Overall, the model appears relatively accurate: the average percentage error is 7.1%. However, the mean absolute error is $351281, which is concerning as the average price in the test set is $689606. This may be the result of high errors for more expensive houses, as the absolute errors on more expensive houses can be greater while having the same percentage error. Indeed, in Figure X and Y, we see that absolute error is significantly higher for homes with high observed prices, but that percent error is consistently low, at less than 10% on average.
```{r Test the regression, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
#Test regression on miami.test
miami.test <-
  miami.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(reg.training, miami.test), #751571.5
         SalePrice.Error = SalePrice.Predict - SalePrice, #60608.35
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice), #363363
         SalePrice.APE = SalePrice.AbsError / SalePrice) %>% #0.05589877  #corrected 
  filter(SalePrice < 5000000) 

#Mean error and APE 432255.1   0.3407089   443611.7 0.1643853 467809 -0.0241117
  #AD: what are the values in line above?
mean(miami.test$SalePrice.AbsError, na.rm = T)#[1] 351280.6
mean(miami.test$SalePrice.APE, na.rm = T)#[1] 0.7101703
mean(miami.test$SalePrice.Predict, na.rm = T)#[1] 770165.4


#AD: absolute errors might be high becuase we have large errors on very expensive houses. (APE is low)
ggplot(data = miami.test) +
  geom_point(aes(x = SalePrice, y = SalePrice.AbsError)) +
  labs(title = "Figure X: Observed Sale Price and Absolute Error") +
  plotTheme()

ggplot(data = miami.test) +
  geom_point(aes(x = SalePrice, y = SalePrice.APE)) +
  labs(title = "Figure Y: Observed Sale Price and Absolute Percent Error") +
  plotTheme()
                                     
```

## Generalizability
In addition to understanding the accuracy of our model, we need to make sure that it is generalizable and can work with different test sets. To do this, we run a K-folds test to test the model on different segments of our test set. 

We see in Table 3 that Fold75, one of the 100 partitioned segments of our data, has a R^2 of 0.78 and a mean average error of $350348.9. While the R^2 is high, the error is more than half of the average predicted price across all folds. (add more analysis)

```{r Fig 7: TABLE of MAE and MAPE for single k- fold test set, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
#K Folds test
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)


reg.cv <- 
  train(SalePrice ~ ., data = st_drop_geometry(housesKnown) %>% 
                                dplyr::select(SalePrice, ActualSqFt, LotSize, Zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType), 
     method = "lm", trControl = fitControl, na.action = na.pass)


#k-fold function online
kfold.MLR = function(fit,k=10,data=fit$model) {    
  sum.sqerr = rep(0,k)
  sum.abserr = rep(0,k)
  sum.pererr = rep(0,k)
  y = fit$model[,1]
  x = fit$model[,-1]
  n = nrow(data)
  folds = sample(1:k,nrow(data),replace=T)
  for (i in 1:k) {
    fit2 <- lm(formula(fit),data=data[folds!=i,])
    ypred = predict(fit2,newdata=data[folds==i,])
    sum.sqerr[i] = sum((y[folds==i]-ypred)^2)
    sum.abserr[i] = sum(abs(y[folds==i]-ypred))
    sum.pererr[i] = sum(abs(y[folds==i]-ypred)/y[folds==i])
  }
  cv = return(data.frame(RMSEP=sqrt(sum(sum.sqerr)/n),
                         MAE=sum(sum.abserr)/n,
             MAPE=(sum(sum.pererr)/n)*100))
}

#Table of MAE and MAPE
length(reg.cv$resample$MAE)
min(reg.cv$resample$MAE)#237439.9
reg.cv$resample[75,]  #AD: we also need standard deviation and MAPE

reg.cv.rs.min %>%
  gather(Variable, Value) %>%
  #filter(Variable == "MAE" | Variable == "RMSE") %>%
  group_by(Variable) %>%
    spread(Variable, Value) %>%
    kable() %>%
   kable_styling()  #AD: How to add title? "Table 3: Regression Results of One Test Set"
```
For more context, Figure X shows that most test sets have errors around $400,000, but that a few outliers skew the average error upward. Additionally, if the model is generalizable, it should have a clustered distribution of errors. As this is not the case, it appears that the model is not predicting consistently across groups of houses.
```{r Fig 8: HISTOGRAM results of cross-validation tests, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}
ggplot(reg.cv$resample, aes(x=MAE)) +
  geom_histogram() +
  labs(title = "Figure X: Mean Average Error in Cross Validation Tests")
  plotTheme()

```
## Do errors cluster?
One way to investigate the reason for the model's inconsistency is to look at how it treats houses across spatial scales.  
```{r Spatial Lag, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
library(knitr)
library(kableExtra)
library(scales)

coords <- st_coordinates(housesKnown)
neighborList <- knn2nb(knearneigh(coords, 5)) #5 nearest neighborhoods

spatialWeights <- nb2listw(neighborList, style="W") #not sure what is W here
housesKnown$lagPrice <- lag.listw(spatialWeights, housesKnown$SalePrice)
plot(housesKnown$SalePrice, housesKnown$lagPrice)


coords.test <-  st_coordinates(miami.test)
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W")


```

In Figure XX, we can see that residuals are evenly distributed spatially. This suggests that the low generalizability of the data is not due to spatial processes, but rather other factors. 
```{r 10: MAP of residuals for test set, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}

#10.1 Map of test set residuals
library(modelr)

miami.test$resid <- 
  miami.test %>%
  as_data_frame() %>%
  add_residuals(., reg.training, var = "resid") %>%
  dplyr::select(resid, Folio) %>%
  pull(resid)

ggplot() +
geom_sf(data = acs, fill = "gray90", colour = "white") +
    geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = miami.test, aes(colour = q5(resid))) +
  scale_colour_manual(values = palette5) +
 labs(title = "Figure X.X: Test Set Residual Errors", subtitle = "Miami and Miami Beach, FL") +
  mapTheme()

```

To discern with more certainty the effect of spatial processes on our errors, we can look at spatial lags, or the clustering of prices and errors. In Figure 10.2, we can see that neighboring houses' price estimate errors do not increase with an individual house's errors. This again tells us that most of our errors are not spatial in nature.
```{r 10.2: Spatial lag, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}
#10.2 Plot: spatial lag in errors

miami.test %>%                
  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%  
  ggplot(aes(lagPriceError, SalePrice)) +
  geom_point() +
  stat_smooth(aes(lagPriceError, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800")+
  labs(title = "Figure 10.2.1: Spatial Lag of Price") +
  plotTheme() + theme(plot.title = element_text(size = 18, colour = "black")) 

#miami.test %>% 
#  mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%   
#  ggplot(aes(lagPriceError, SalePrice.Error)) +
#  geom_point() +
#  stat_smooth(aes(lagPriceError,SalePrice.Error), 
#             method = "lm", se = FALSE, size = 1, colour="red")+
#    labs(title = "Figure 10.2: Spatial Lag of Errors") +
#  plotTheme() + theme(plot.title = element_text(size = 18, colour = "black")) 

```

Finally, we can use a Moran's I test to gain further insight into the spatial autocorrelation of our model errors. If our model errors are not influenced by spatial processes, we should see a Moran's I of 0. Our Moran's I value is less than 0.2, which confirms that we have very minimal spatial clustering of errors.
```{r 10.3:Morans I, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}

moranTest <- moran.mc(miami.test$SalePrice.Error,
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Figure 10.3 Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count",
       caption="Public Policy Analytics, Figure x.x") +
  plotTheme()
```


## Accounting for neighborhood variance
While our spatial clustering of errors is low, we can increase our model's accuracy by accounting for it. Here, we add neighborhood as a feature in our model to account for the differences in sale price across neighborhoods. Table XX tells us that when we account for neighborhood effects, we actually slightly increase the absolute error but we decrease the absolute percentage error. This may mean that the neighborhood model works best for lower priced homes, and that it increased the error in expensive ones. It also tells us that our Baseline model already accounted for spatial disparities through our use of open data features.
```{r Neighborhood Regression, message=FALSE, warning=FALSE, include=TRUE, results='hide'}
left_join(
  st_drop_geometry(miami.test.nhood) %>%
    group_by(neighborhood) %>%
    summarize(meanPrice = mean(SalePrice, na.rm = T)),     
  mutate(miami.test, predict.fe =
                        predict(lm(SalePrice ~ neighborhood, data = miami.test.nhood),
                        miami.test)) %>%
    st_drop_geometry %>%
    group_by(neighborhood) %>%
      summarize(meanPrediction = mean(predict.fe))) %>%
      kable() %>%
  kable_styling()


#Make new neighborhood regression
reg.nhood <- lm(SalePrice ~ ., data = as.data.frame(miami.training) %>% 
                                 dplyr::select(neighborhood, SalePrice, ActualSqFt, LotSize, Zoning, Stories.cat, Bath.cat, Pool, medHHInc, Dock, Bed.cat, middleCatch, age, pctVacant, pctRenterOcc, monthhousingcost, Patio, foodEstablishments, timeToWork, metromoverStops, metrorailStops, parkArea, floodInsureType))
summary(reg.nhood)

#Outcomes
miami.test.nhood <-
  miami.test %>%
  mutate(Regression = "Neighborhood Effects",
         SalePrice.Predict = predict(reg.nhood, miami.test), #613237.3
         SalePrice.Error = SalePrice - SalePrice.Predict, # -108442.4      
         SalePrice.AbsError = abs(SalePrice - SalePrice.Predict), # 491238.7
         SalePrice.APE = (abs(SalePrice - SalePrice.Predict)) / SalePrice)%>% #0.7109973
  filter(SalePrice < 5000000)
summary(miami.test.nhood)

#Check accuracy
bothRegressions <-
  rbind(
    dplyr::select(miami.test, starts_with("SalePrice"), Regression, neighborhood) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
    dplyr::select(miami.test.nhood, starts_with("SalePrice"), Regression, neighborhood) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))   

st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -neighborhood) %>%
  filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value, na.rm = T)) %>%
    spread(Variable, meanValue) %>%
    kable(caption = "Table XX: Neighborhood Effect on Error")

```

In Figure XX, we can see the effect of neighborhood model on the accuracy of our predictions. As suspected, the neighborhood model fits the data only marginally better. For some of the higher priced homes, however, it appears that adding neighborhood as a feature caused an overestimation of price. 
```{r Fig 9:PLOT predicted prices as a function of observed prices, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}

bothRegressions %>%
  dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
    ggplot(aes(SalePrice, SalePrice.Predict)) +
  geom_point() +
  stat_smooth(aes(SalePrice, SalePrice),
             method = "lm", se = FALSE, size = 1, colour="#FA7800") +
  stat_smooth(aes(SalePrice.Predict, SalePrice),
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Figure XX: Predicted Sale Price as a function of Observed Price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme() + theme(plot.title = element_text(size = 18, colour = "black"))

```

Finally, Figure XX shows the prices that we predicted for the set of 3503 Miami area homes. As we saw with the known home sale prices, our predicted prices are highest close to the shoreline and on Miami Beach. 
```{r 11: map of predicted values, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}
#876 rows
#filter by toPredict = 1
housesPredictions <-                   
  houses %>%
  mutate(prediction = predict(reg.nhood, houses),
         team_name = 'Panda')

predictions <- housesPredictions[,c("Folio", "prediction", "team_name", "toPredict")] %>%
  st_drop_geometry() %>%
  filter(toPredict == 1) %>%
  dplyr::select(-toPredict)

# write.csv(predictions, "PANDA.csv") #"The column names MUST to be "prediction", "Folio", and "team_name"" - piazza


#Map values
ggplot() +
  geom_sf(data = acs, fill = "gray90", colour = "white") +
    geom_sf(data = water, fill = "light blue", colour = "light blue") +
  geom_sf(data = housesPredictions, aes(colour = q5(prediction))) +
 scale_colour_manual(values = palette5) +
 labs(title = "Figure X.X: Predicted Sale Price Values", subtitle = "Miami and Miami Beach, FL") +
 # facet_wrap(~toPredict) +
  mapTheme()
```

In Figure XX, we can see the locations of our errors. Overall, it appears that mean average percentage error is lower in Miami Beach, where sale prices are also higher. It is highest in the center of Miami, and also relatively low on the Miami shoreline. This suggests that our errors were smaller in neighborhoods with higher observed sale prices.
```{r 12: Test set predictions, provide a map of MAPE by neighborhood, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}
#Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood
names(bothRegressions)[names(bothRegressions) == "neighborhood"] <- "neighborhood"
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, neighborhood) %>%
  summarise(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
  ungroup() %>%
  left_join(nhoods) %>%
    st_as_sf() %>%
   ggplot() +
    geom_sf(data = water, fill = "light blue", colour = "light blue") +
      geom_sf(colour = "gray", aes(fill = q5(mean.MAPE))) +
      scale_fill_manual(values = paletteMap) +
  labs(title = "Figure XX: Mean Average Percentage Error by Neighborhood") +
#   geom_text(data=nhoods, aes(label=neighborhood), alpha = 0.75, size = 2) +
      mapTheme()

```

Figure XX confirms this trend. We can see clearly that with one drastic exception, neighborhoods with lower mean prices have higher mean average percentage errors. 
```{r 13: Scatterplot of MAPE by neighborhood as a function of mean price by neighborhood, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}

scatter_hood <-
    miami.test.nhood %>%
    group_by(neighborhood) %>%
    dplyr::select(neighborhood, SalePrice.APE, SalePrice.Predict)

mean_sca_hd <-
  scatter_hood %>%
  group_by(neighborhood) %>%
  summarise_at(vars("SalePrice.APE", "SalePrice.Predict"), mean)

plot(mean_sca_hd$SalePrice.Predict, mean_sca_hd$SalePrice.APE, main="Figure XX: MAPE by Neighborhood and Mean Price by Neighborhood", xlab="Mean Price by Neighborhood", ylab="MAPE by neighborhood") +
  plotTheme()

#https://r-graphics.org/recipe-scatter-labels or we can use the method below:

#scatter_mae_mean <- ggplot(mean_sca_hd, aes(x = SalePrice.Predict, y = SalePrice.APE)) +
#    geom_point() +
#  plotTheme()

#scatter_mae_mean +    #AD: this is cool but we can't really see which dot is which neighborhood
#  annotate("text", x = 820000, y = -11.7, label = "UPPER EASTSIDE") +
#  annotate("text", x = 330000, y = -6.5, label = "LITTLE HAITI")+
#  annotate("text", x = 1300000.82, y = 6.85325579, label = "NAUTILUS")+
#  annotate("text", x = 631682.30, y = 3.57218432, label = "BISCAYNE POINT")+
#  annotate("text", x = 1557855.78, y = 2.81996319, label = "LA GORCE")+
#  annotate("text", x = 2031665.51, y = 1.95167248, label = "WYNWOOD - EDGEWATER")+
#  annotate("text", x = 890031.19, y = -2.16394539, label = "OVERTOWN")

```

## Does this model work equally for different demographic groups?
The variation in the MAPE of different neighborhoods suggests that our model may have limited generalizability across different types of neighborhoods. To test this, we will look at how well it predicts prices across neighborhoods with different racial and income compositions. In Figure XX is the racial and income context of Miami. 

Table XX and XX confirm that this model applies slightly differently for different demographics. In Majority non-white neighborhoods, MAPE is 30% higher than in majority white neighborhoods. Similarly, MAPE is 27% higher in low income neighborhoods than in high income ones. 
```{r 14: race and income, fig.height=5, fig.width=8, message=FALSE, warning=FALSE, include=TRUE}
#RACE & INCOME
  #make new layer
acsRaceIncome <-
  acs %>%
  mutate(raceContext= ifelse(pctWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(medHHInc > 49256.56, "High Income", "Low Income"))
#context
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(acsRaceIncome), aes(fill = raceContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
    labs(title = "Race Context") +
    mapTheme() + theme(legend.position="bottom"),
  ggplot() + geom_sf(data = na.omit(acsRaceIncome), aes(fill = incomeContext)) +
    scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
    labs(title = "Figure XX: Income Context of Miami and Miami Beach") +
    mapTheme() + theme(legend.position="bottom"))

#tables
st_join(bothRegressions, acsRaceIncome) %>%
  filter(!is.na(raceContext)) %>%
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  kable(caption = "Table XX: MAPE by neighborhood racial context")

st_join(bothRegressions, acsRaceIncome) %>%
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  kable(caption = "Table XX: MAPE by neighborhood income context")


```

# Discussion
In conclusion, our model is effective at predicting the distribution of home sale prices in the area as the pattern of predicted prices matches that of recent known home sale prices. However, it is limited in its ability to generalize across different types of neighborhoods. These errors are likely attributable to the fact that we used mainly positive home attributes and neighborhood amenities in our model. If we used more attributes such as poverty rate and renter occupancy rate, our model may have been better at modeling majority-minority and low-income neighborhoods.

We have many interesting variables which had unexpected relationships with salePrice. As expected, distance from the shore correlates negatively with sale price, however the correlation was much weaker than expected because it only seemed to matter for the first mile from water. By contrast, floodInsureType correlated quite strongly with sale price. This feature comes from FEMA's rating of flood risk for different neighborhoods, and unlike our expectation, it was not only based on distance from the shoreline. 
  
In general, we found that houses' internal features were the strongest predictors of sale price. In addition, census features such as percent vacancy in a tract, median household income, and travel time to work correlated strongly with sale price. Contrary to our expectations, distance to water, food establishments, businesses, school catchment areas, and park space are less correlated with home sale price. 




Describe the error in your predictions?  According to your maps, could you account the spatial variation in prices?  Where did the model predict particularly well? Poorly? Why do you think this might be?
